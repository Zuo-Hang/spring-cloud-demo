微服务：

服务注册与发现

服务调用

服务熔断

负载均衡

服务降级

服务消息队列

配置中心管理

服务网关

服务监控

全链路追踪

自动化部署

服务定时任务调度操作

spring cloud：一站式解决方案

服务的分类：

无业务基础服务：全局配置、ID自增器、短链接服务、文件存储服务、身份验证、邮件短信平台、语音回拨……

业务型基础模块：用户中心、账户中心、支付中心、信审中心、信息抓取系统、消息中心、活动广告、CMS……

最终springCloud通过网关调用负载均衡的每一个微服务。

版本：

imooc6666



理论：

CAP理论：（悖论）

* 数据一致性（consistency）：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有的读操作都不能读到这个数据，对调用者而言数据具有强一致性（strong consistency）
* 服务可用性（availability）：所有读写请求在一定时间内得到相应，可终止、不会一直等待
* 分区容错性（partition-tolerance）：在网络分区的情况下，被分隔的节点仍能正常对外服务

如果选择了CA而放弃了P，那么当发生分区现象时，为了保证C，系统需要禁止写入，当有写入请求时，系统返回error（例如，当前系统禁止写入），这又和A冲突了，因为A要求返回no error和no timeout。因此，分布式系统理论上不可能选择CA架构，只能选择CP或AP架构。

反证：

如果CAP三者可同时满足，由于允许P（网络分区）的存在，则一定存在节点之间丢包，如此则不能保证C

因为允许分区容错，写操作可能在1上成功，在节点2上失败，这时候对于client1（读取1节点）和client2读取2节点，就会读取到不一致的值，出现不一致的状况。如果要保持一致，写操作必须同时失败，也就是降低系统的可用性。

Base理论：（妥协）

cap理论的一种妥协，由于cap只能二取其一，base理论降低了发生分区容错时对可用性和一致性的要求。

1.基本可用：允许可用性降低（可能相应延长、可能服务降级）

2.软状态：指允许系统中的数据存在中间状态，并认为中间状态不会影响系统整体可用性。

3.最终一致性：节点数据同步可以存在延迟，但在一定的期限后必须达成数据的一直，状态变为最终状态。（很多种方案：因果一致、会话一致、单调一致……多种一致性模型）读写一致性

##### 数据一致性模型有哪些

强一致性：当更新操作完成之后，任何多个后续的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就能保证读到什么。根据cap理论，这种实现需要牺牲可用性。

弱一致性：系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。

最终一致性：最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致性，而不需要实时保证系统数据的强一致性。达到最终一致性的时间就是不一致性窗口时间，在没有故障的前提下，不一致性窗口的时间主要受通信延迟、系统负载和复制副本的个数影响。

最终一致性模型根据其提供的不同保证可以划分为更多的模型，包括因果一致性和会话一致性等。



因果一致性：要求有因果关系的操作顺序执行得到保证，非因果关系的操作顺序则无所谓。

进程A在更新完某个数据后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取进程A更新后的最新值，并且如果进程B要对该数据进行更新操作的话，务必基于进程A更新后的最新值。

在微博或朋友圈进行评论的时候，比如你在朋友圈发了一整照片，朋友评论了，而你对朋友的评论进行了回复，这条朋友圈的显示中，你的回复必须在朋友之后，这是一个因果关系，而其他没有因果关系的数据，可以允许不一致。

会话一致性：将对系统数据的访问狂顶在了一个会话当中，约定了系统能过够保证在同一个有效绘画中实现“读自己所写”的一致性，就是你的一次访问中，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。实际开发过程中有分布式Session一致性问题，可以认为是会话一致性的一个应用。

单调一致性：

读写一致性：



##### 选举算法（主从架构中）Quorum、QARO机制

WARO：一种简单的副本控制协议，写操作时、只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。优先保证读、任何节点读到的数据都是最新数据，牺牲了更新服务的可用性、只要有一个副本宕机了，写服务就不会成功。但只要有一个节点存活，仍能提供读服务。（对应cpa的强一致性）kafka ack.o之中有使用其优化。

Quorum机制：10个副本，一次成功更新了三个，那么至少需要读取八个副本的数据（10-3+1），可以保证读取到了最新的数据。无法保证强一致性，也就是无法保证任何实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据。需要配合一个获取最新成功提交的版本号的metadata服务，这样可以确定最新已经耿工提交的版本号，然后从已经读取到的数据中就可以确认锥心写入的数据。

##### Paxos算法：

思想，可以认为是一种协议。解决强一致性

Paxos算法解决的事实一个分布式系统如何让就某个值（决议）达成一致。一个典型的场景是，在一个分布式数据库系统中，如果每个节点的初始状态一致，每个节点执行相同的操作序列（指令+顺序），那么他们最后能够得到一个一致的状态。为了保证每个节点执行相同的操作序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。在Paxos算法中，有三种角色：Proposer（提议者）、Acceptor（接受者）、Learners（记录员）

* Proposer提议者：只要Proposer发的提案Proposer被半数以上的Acceptor接受，Proposer就认为该提案的value被选定了。
* Acceptor接受者：只要Acceptor接受了某个提案，Acceptor就认为该提案的value被选定了
* Learner记录员：Acceptor告诉Learner哪个value被选定，Learner就认为哪个value被选定。

Paxos算法分为两个阶段，具体如下：

阶段一（preprae）：

1）Proposer接受client请求或者发现本地有未提交的值，选择一个提案编号N，然后向半数Acceptor发送编号为N的Prepare请求。

2）Acceptor收到一个编号为N的Prepare请求，如果该轮Paxos

* 本节点已经有已提交的Value记录（持久化的），对比记录的编号和N，其值大于N则拒绝回应，否则返回该记录value及编号
* 没有已提交记录（没有持久化，但是接受了N1的其他提议），判断本地是否有编号N1，N1>N、则拒绝相应，否则将N1改为N（如果没有N1，则记录N），并相应prepare

阶段二（accept）：

1）如果proposer收到半数以上Acceptor对其发出的编号为N的Prepare相应的请求，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。V就是收到的相应中编号最大的Value，如果相应中不包含任何Value，那么V由Proposer自己决定。

2）如果Acceptor收到一个针对编号为N的提案的Accept请求，Acceptor对比本地的记录编号，如果小于等于N则接受该值，并提交记录Value。否则拒绝请求。



Proposer如果收到的大多数Acceptor相应，则选定Value值，并同步给Leaner，使未响应的Acceptor达成一致

![](分布式微服务.assets\QQ截图20210407182946.png)

活锁：accept时被拒绝，加大N，重新accept，此时另一个proposer也进行相同的操作，导致accept一致失败，无法完成算法。

multi-paxos：区别于paxos值时确定一个值，multi-paxos可以确定多个值，收到accept请求后，则一定时间内不再accept其他节点的请求，以此保证后续的编号不需要经过preprae确认，直接进入到accept操作。此时该节点成为leader，直到accept被拒绝，重新发起prepare请求竞争leader资格。



简述raft算法：

概念：

* 分布式一致性算法：raft会先选举出leader，leader完全负责repliated log的管理。leader负责接受所有的客户端更新请求，然后复制到foller节点，并在“安全”的时候执行这些请求。如果leader故障，follwes会重新选举出新的leader
* 三种状态：一个节点任何一时刻处于三者之一
  * leader：处理所有的客户端请求（如果客户端将请求发给Follower，Follower将请求重定向给Leader）
  * follower：不会发送任何请求，只会简单地响应来自leader或Candidate的请求
  * candidate：用于选举残生新的leader（候选人）
* term：人气，leader产生到重新选取的一任期，每个节点都维持着当前的任期号
  * term时递增的，存储在log日志的entry中，代表当前entry时在哪一个term时期写入
  * 每个任期只能由一个leader或者没有（选举失败）
  * 每次rpc通信时传递该任期号，如果rpc收到任期号大于本地的、切换follower，小于本地人气好则返回错误信息
* 两个Rpc通信：
  * RequestVote Rpc ：负责选举，包含参数lastIndex、lastTerm
  * AppendEntries Rpc：负责数据交互
* 日志序列：每一个节点维持着一份持久化Log，通过一致性协议算法，保证一个节点中的log保持一致，并顺序存放，这样客户端就可以在每一个节点中读取到相同的数据。
* 状态机：日常序列同步到多数节点时，leader将该日志提交到状态机，并在下一次心跳通知所有节点提交状态机（携带最后提交的lastIndex）



何时触发选举：

* 集群初始化时，都是follower，随机超时，变成candidate，发起选举
* 如果follower在election timeout内没收到来自leader的心跳，则主动触发选举

选举过程：发出选举的节点角度

1、增加节点本地的term，切换到candidate状态

2、投自己一票

 		其他节点的投票逻辑：每个节点同一任期最多只能投一票，候选人知道的信息不能比自己少（通过副本日志和安全机制），先来先得

3、并行给其他节点发送RequestVote Rpcs（选举请求）、包含term参数

4、等待回复

​	4.1、收到majority（大多数）的投票，赢得选举，切换到leader状态，立刻给所有系欸但发心跳消息

​	4.2、被告知别人当选，切换到follower状态。（原来的leader对比term，比自己大，转换到follower状态）

​	4.3、一段时间没收到majority和leader的心跳通知，则保持candidate、重新发出选举



日志序列同步：日志需要存储在磁盘持久化，崩溃可以从日志恢复

1、客户端发送命令给Leader。

2、Leader把日志条目加到自己的日志序列里。

3、Leader发送AppendEntries Rpc请求给所有的follower。携带了prevLogIndex、prevLogTerm follower收到后，进行日志序列匹配

* 匹配上则之家到自己的日志序列
* 匹配不上则拒接请求，leader将日志index调小，重新同步直至匹配上，follower将leader的日志序列覆盖到本地



 一旦新的日志序列条目变成majority的了，将日志序列应用到状态机中

* Leader在状态机里提交自己日志序列条目，然后返回结果给客户端
* Leader下次发送AppendEntries Rpc时，告知佛来了微软已经提交的日志序列条目信息（lastIndex）
* follower收到RPC后，提交到自己的状态机里

提交状态机时，如果term为上一任期数据一起提交，否则可能出现覆盖已提交状态机的日志



新选举出的leader一定拥有所有已提交状态机的日志条数

* leader在当日志序列条目已经复制到大多数follower机器上时，才会提交日志条目。
* 而选出的leader的logIndex必须大于等于大多数节点，因此leader肯定有最新的日志



安全原则：

* 选举安全原则：对于一个给定的人气好，最多只会有一个leader被选取出来
* 状态机安全原则：如果一个leader已经在给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会提交一个不同的日志
* 领导人完全原则：如果某个日志条目在某个人气好中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中
* 领导人只附加原则：领导人结对不会删除或者覆盖自己的日志，只会增加
* 日志匹配原则：如果两个日志在相同的索引位置的日志的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同



##### 简述Zab协议

ZAB协议是为夫不是协调服务zookeeper专门设计的一种支持崩溃恢复的原子广播协议，实现分布式数据一致性

所有客户端的请求都是书写到Leader进程中，然后由leader同步到其他系欸但，成为follower。在集群数据同步的过程中，如果出现follower节点崩溃或者leader进程崩溃，都会通过zab协议来保证数据一致性

zab协议包含两种模式：崩溃恢复和消息广播。

消息广播：

集群中所有的事物请求都由leader节点来处理，其他服务器为follower，leader将客户端的事物请求转换为事物proposal，并且将proposal分发给集群中其他的所有follower

完成广播之后，leader等待follwer反馈，当由过半数的follower反馈信息后，leader将再次向集群内的follower广播commit信息，commit信息就是确认将之前的proposal提交。

leader节点的写入时一个两部操作，第一步是广播事务操作，第二步是广播提交操作，其中过半数是指反馈节点数>=N/2+1，N是全部follower节点数量。



崩溃恢复：

* 初始化集群，刚刚启动的时候
* leader崩溃，因为故障宕机
* leader失去半数的机器支持，与集群中超过一半的节点断连

此时开启新一轮的leader选举，选举产生的leader会与过半的follower进行同步，使数据一致，当与过半的机器同步完成后，就退出恢复模式，然后进入消息广播模式。



整个Zookeeper集群的一致性保证就是在上面两个状态之间切换，当leader服务正常时，就是正常的消息广播模式；当leader不可应时，则进入崩溃恢复模式，崩溃恢复阶段会进行数据同步，完成以后，重新进入消息广播阶段。



Zxid 是zab协议的一个事物编号，zxid是一个64位的数字，其中低32位是一个简单的单调递增计数器，针对客户端每一个事物请求，计数器加1；而高32位则代表leader周期年代的编号。

leader周期（epoch），可以理解为当前集群所处的年代或者周期，每当有一个新的leader选举出现时，就会从这个leader服务器上取出其本地日志中最大事物的zxid，并从中读取epoch值，然后加一，以此作为新的周期id。高32位代表了每代leader的唯一性，低32位则代表了每代leader中事物的唯一性。



zab节点的三种状态：

following：服从leader的命令

leading：负责协调事物

election、looking：选举状态



zab协议的一次选举肯定可以选出一个leader，但是reft协议不一定。



##### 负载均衡策略：

1.轮询法：

将请求按顺序轮询地分配到后端服务器上，它均衡的对待后端的每一台服务器，而不关心服务器实际链接数和当前的系统负载。

2.加权轮询法

不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此他们的抗压能力也不同。给配置高、负载低的机器分配更高的权重，让其处理更多的请求；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好的处理这一问题，并将请求顺序且按照权重分配到后端。

3.随机法

通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。

4.加权随机法

与加权轮询一样，加权随机法也更具后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。

5.源地址哈希法

源地址哈希法的思想是根据客户端的ip地址，通过哈希函数计算得到一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客户端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一ip地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。（解决了共享session的问题，但割裂了集群）

6.最小连接数法

最小链接数算法比较灵活和只能，由于后端服务器的配置不尽相同，对于请求的处理有块有慢，它是根据后端服务器当前的连接情况，动态的选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能的提高后端服务的利用率，将负责合理地分流到每一台服务器。

状态机：数据库、日志、



谁的值最先被大多数Accept，谁的值就会被持久化。

##### 集群、分布式、SOA、微服务的概念及区别

集群：不同服务器部署同一套应用对外提供访问，实现服务的负载均衡或者互备（热备、主从等），指同一种组件的多个实例，形成的逻辑上的整体。单个节点可以提供完整的服务。集群是物理形态。

分布式：服务的不同模块部署在不同的服务器上，单个节点不能提供完整服务，需要多个节点协调提供服务（也可以是相同组件部署在不同的节点，但节点间通过交换信息写作提供服务），分布式强调的是工作方式

SOA：面向服务的架构，一种设计方法，其中包含多个服务，服务之间通过相互依赖最终提供一系列的功能。一个服务通常以独立的形式存在于操作系统进程中。各个服务之间通过网格调用。

* 中心化实现：ESB（企业服务总线），各服务通过ESB进行交互，解决异构系统之间的连通性，通过协议转换、消息解析、消息路由把服务提供者数据传送到服务消费者。很重，有一定的逻辑，可以解决一些公用逻辑的问题。
* 去中心化实现：微服务

微服务：在SOA上做的生化，微服务架构强调的一个重点是业务需要彻底的组件化和服务化，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成

服务单一职责

轻量级通信：去掉ESB总线，采用RestApi通信

##### 分布式系统的设计目标

可扩展性：通过对服务、存储的扩展，来提高系统的处理能力，通过对多台服务器协同工作，来完成单台服务器无法处理的任务，尤其是高并发或者大数据量的任务。

高可用：单点不影响整体，单点故障指系统中某个组件一旦失效，会让整个系统无法工作

无状态：无状态的服务才能满足部分机器宕机不影响全部，可以随时进行扩展的需求

可管理：便于运维，出问题能不能及时发现定位

高可靠：同样的请求返回相同的数据；更新能够持久化；数据不会丢失

##### 分布式事物有哪些解决方案

基于XA协议：两阶段提交和三阶段提交，需要数据库层面支持

基于事物补偿机制的：TCC，基于业务层面实现

本地消息表：基于本地数据库+mq，维护本地状态（进行中），通过mq调用服务，完成后相应一条消息回调，将状态改成完成。需要配合定时任务扫表，重新发送消息调用服务，需要保证幂等

基于事物消息：mq

对比两阶段、三阶段有哪些改进

两阶段协议：

![QQ截图20210422120227](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210422120227.png)

第一阶段（prepare）：每个参与者执行本地事物但不提交，进入ready状态，并通知协调者已经准备就绪

第二阶段（commit）：当协调者确认每个参与者都ready后，通知参与者进行commit操作；如果有参与者fail，则发送rollback命令，各参与者做回滚。

问题：

* 单点故障：一旦事务管理器出现故障，整个系统不可用（参与者都会阻塞住）
* 数据不一致：在阶段二，如果事务管理器只发送了部分commit消息，此时网络发生异常，那么只有部分参与者接受到commit消息，也就是说只有部分参与者提交了事物，使得系统数据不一致
* 相应时间较长：参与者和协调者资源都被所著，提交或者回滚之后才能释放
* 不确定性：当协事物管理器发送commit之后，并且此时只有一个参与者收到了commit，那么当该参与者与事物管理器同时宕机之后，重新选举的事物管理器无法确定该条消息是否提交成功

三阶段协议：主要是镇墩两阶段的优化，解决了2PC单点故障的问题，但是性能问题和不一致问题依然没有根本解决

![QQ截图20210422180837](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210422180837.png)

引入超时机制解决参与者阻塞的问题，超时后本地提交，2PC只有协调者有超时机制

* 第一阶段：canCommit阶段，协调者询问事物参与者，是否有能力完成此次事物。
  * 如果都返回yes，则进入第二阶段
  * 如果有一个返回no或等待响应超时，则中断事物，并向所有参与者发送abort请求
* 第二阶段：PreCommit阶段，此时协调者会向所有的参与者发送PreCommit请求，参与者收到后开始执行事物操作。参与者执行完事物操作后（此时属于未提交事物的状态），就会向协调者反馈“Ack”表示我已经准备好提交了，并等待
* 第三阶段：DoCommit阶段，在阶段二中如果所有的参与者节点都返回了Ack，那么协调者就会从“预提交状态”转变为“提交状态”。然后向所有的参与者发送“doCommit”请求，参与者节点在搜到请求后会各自执行事物提交操作，并且向协调者节点反馈“Ack”消息，协调者收到所有参与者的Ack消息后完成事物。相反，如果有一个参与者节点未完成preCommit的反馈或者反馈超时时，那么协调者都会向所有的参与节点发送abort请求，从而中断事物。



### spring cloud



总的父工程：

pom

project

​	molded：

​	



new project

聚合总父工程名字

Maven选版本

工程名字

字符编码

注解生效激活

Java编译版本选8

file type过滤



1.建module

2.改pom

3.写yml

4.主启动类

5.业务类

热部署：

![QQ截图20210411234925](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210411234925.png)

![QQ截图20210411234829](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210411234829.png)

0基础-初级-中级-高级

![QQ截图20210413122028](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210413122028.png)

配置中心还有一个携程的 apollo

#### Eureka服务注册与发现

什么是服务治理：

Spring cloud封装了Netflix公司开发的Eureka模块来实现服务治理

在传统的RPC远程调用中，管理每个服务与服务之间依赖关系比较复杂，管理比较复杂，所以需要使用服务治理，管理服务与服务之间的依赖关系，可以实现服务调用、负载均衡、容错等，实现服务发现与注册。

什么是服务注册与发现：

Eureka采用CS的设计架构，Eureka service作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务，使用Eureka的客户端连接到Eureka service并维持心跳链接。这样系统的维护人员就可以通过Eureka service来监控系统中各个微服务是否正常运行。

在服务注册与发现中，有一个注册中心。当服务器启动的时候，会把当前自己服务器的信息，比如：服务地址、通讯地址等以别名的方式注册到注册中心上。另一方面（消费者/服务的提供者）以该别名的方式去注册中心上获取到实际的服务通讯地址，然后再实现本地RPC调用RPC远程调用框架核心设计思想：在于注册中心，因为使用注册中心管理每个服务与服务之间的一个依赖关系（服务治理概念）。在任何RPC远程框架中，都会有一个注册中心（存放服务地址相关信息（接口地址））

![QQ截图20210413123842](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210413123842.png)

服务需要和注册中心维持心跳。

集群避免单点故障。

Eureka包含两个组件：Eureka server和Eureka client

Eureka server：提供服务注册服务

各个微服务节点通过配置启动后，会在Eureka server中进行注册，这样Eureka server中的服务注册表中将存储所有可用的服务节点的信息，服务节点的信息可以在界面中观看到。

服务注册：将服务信息注册到注册中心

服务发现：从注册中心上获取服务信息

实质：存储key服务名 取value调用地址

Eureka client：通过注册中心进行访问

是一个Java客户端，用于简化Eureka server的交互，客户端同时具备一个内置的、使用轮询（round-robin）负载算法的负载均衡器。在应用启动后，将会向Eureka server发送心跳（默认周期为30秒）。如果Eureka server在多个心跳周期内没有接收到某个节点的心跳，Eureka server将会从服务注册中心将这个节点移除（默认90秒）。

1.先启动eureka注册中心

2.启动服务提供者payment支付服务

3.支付服务启动后会把自身信息（比如服务地址以别名的方式注册进eureka）

4.消费者order服务在需要调用接口的时候，使用服务名去注册中心获取实际的RPC远程调用地址

5.消费者获取调用地址后，底层实际是利用HTTPClient技术实现远程调用

6.消费者获得服务地址后会缓存在本地jvm内存中，默认间隔30秒更新一次服务调用地址



微服务RPC远程服务调用最核心的是什么？

高可用，试想注册中心只有一个，他出故障了就会导致整个服务环境不可用。所以需要搭建Eureka注册中心集群（互相注册，相互守望），实现负载均衡+故障容错



单机集群的模拟：

更改hosts文件：C:\Windows\System32\drivers\etc

增加：

127.0.0.1 eureka7001.com

127.0.0.1 eureka7002.com

服务发现Discovery：

对于注册进eureka里面的微服务，可以通过服务发现来获取该服务的信息

修改cloud-provider-payment8001的controller

Eureka的自我保护模式：

默认情况下，如果eureka server在一定时间内没有接收到某个服务实例的心跳，Eureka server会将注销实例（默认90秒）。但是当网络故障发生（延迟、卡顿、拥挤）时，微服务与eureka server之间无法正常通信，以上行为变得非常危险——因为微服务本身是健康的，此时本不该注销这个微服务。eureka通过自我保护模式来解决这个问题——当eureka server节点在短时间内丢失过多的客户端时（可能发生了网络分区故障），那么这个节点会进入自我保护模式。

![QQ截图20210413222820](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210413222820.png)

在自我保护模式中，eureka server会保护服务注册表中的信息，不再注销任何的服务实例。

它的设计思想就是宁可保留错误的服务注册信息，也不盲目的注销任何可能健康的服务实例。故自我保护模式是一种宁可保留错误的服务注册信息，也不会盲目注销任何健康的服务。使用自我保护模式可以让eureka集群更加的健壮和稳定。

故障现象：eureka主页报红字

导致原因：某时刻某一个微服务不可用了，Eureka不会立刻清理，依旧会对该微服务的信息进行保存。是属于CAP理论中的AP分支。

为了防止Eureka client可以正常运行，但是Eureka server网络不通的情况下，eureka server不会将eureka client服务剔除

怎么禁止自我保护：



#### springcloud整合zookeeper

![QQ截图20210415113016](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210415113016.png)

1.为stanos安装zookeeper

2.关闭防火墙

3.虚拟机和windows 互相ping ip地址  （Net+桥接）

4.启动zookeeper

5.启动客户端

6.启动服务

服务器的注册是临时节点

#### Consul

Consul是一套开源的分布式服务发现和配置管理系统，由HashiCorp公司用Go语言开发



提供了微服务系统中心的服务治理、配置中心、控制总线等功能。这些功能中的每一个都可以根据需求单独的使用，也可以一起使用以构建全方位的服务网络，总之consul提供了一种完整的服务网格解决方案。

其实Consul的配置基本和zookeeper差不多

Consul可以在官网下载服务器，下载解压完成只有一个consul.exe文件，可以使用命令进行开发模式启动，Consul的服务器端口号为8500

```
consul agent -dev
```

三个注册中心的异同：

| 组件名    | 语言 | CAP  | 服务健康检查 | 对外暴露接口 | web界面 | Spring-cloud集成 |
| --------- | ---- | ---- | ------------ | ------------ | ------- | ---------------- |
| Eureka    | Java | AP   | 可配支持     | HTTP         | 有      | 已集成           |
| Consul    | Go   | CP   | 支持         | HTTP/DNS     | 有      | 已集成           |
| Zookeeper | Java | CP   | 支持         | 客户端       | 无      | 已集成           |

CAP：

![QQ截图20210416121544](C:\Users\zuo_h\Desktop\分布式微服务.assets\QQ截图20210416121544.png)

C：强一致性

A：可用性

P：分区容错性

CAP理论关注粒度是数据，而不是整体系统设计的

cap最多只能较好的满足两个。

CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性、可用性和分区容错性这三个需求。

因此，根据CAP原理将NoSQL数据库分成了满足CA原则、满足CP原则和满足AP原则三大类。

CA：单点集群，满足一致性，高可用性的系统，通常在可扩展性上不强大。

CP：满足一致性，分区容错性的系统，通常可用性不是特别高。

AP：满足可用性，分区容错性的系统，通常可能对一致性要求较低。

--------------------------------------------------深入理解cap中的cp和ap

#### Ribbon负载均衡服务调用

Spring-Cloud-Ribbon是基于NetFlix Ribbon实现的一套客户端负载均衡的工具。

简单来说，Ribbon是NetFlix发布的开源项目，主要功能是提供客户端的软件负载均衡算法和服务调用。Ribbon客户端组件提供一系列完善的配置如链接超时，充实等。简单来说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，Ribbon会自动的帮助你基于某种规则（如简单轮询，随机链接等）去链接这些机器。我们很容易使用Ribbon实现自定义的负载均衡算法。



#### OpenFeign服务接口调用

#### Hystrix断路器

gateWey网关



87

100

## end
